{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import contractions\n",
    "import emot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import seaborn as sb\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACL-14-Short: Brief Description\n",
    "Exclusively tweets.\n",
    "\n",
    "The raw data is in a text file and spread over groups of 3 lines such that for $i \\in [0,nlines/3]$:\n",
    "* line i * 3 --> contains the context\n",
    "* line i * 3 + 1 --> contains the sentiment target\n",
    "* line i * 3 + 2 --> contains the sentiment polarity\n",
    "\n",
    "Data cleaning plan:\n",
    "* remove duplicate columns\n",
    "* remove unnecessary references (http, @mentions, etc., and empty spaces using regex)\n",
    "* clean up contractions (using the contractions library)\n",
    "* clean up emojis, replace encodings by plain english description (using emot)\n",
    "* all text is lowercased\n",
    "* polarities are converted to int.\n",
    "\n",
    "Note that some of these steps have already been taken for ACL-14 and are therefore redundant. However the functions developed will be used to develop a standard ETL pipeline developed and will be applied regardless of a particular data set's preprocessing history. This will insure uniformity across multiple data sets.\n",
    "\n",
    "At the end of this notebook train- and test dataframes will be stored in acl-14's \"clean\" directory as df_train and df_test. They will contain the following columns.\n",
    "* context: left as is;\n",
    "* preprocessed context: context preprocessed follwoign steps listed above;\n",
    "* target: processed as above, where applicable;\n",
    "* polarity: ternary label, stored as integer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_path = Path('/home/ekarakoylu/PROJEX/directed_sentiment_nli')\n",
    "acl_data_path = proj_path /'data/acl-14-short'\n",
    "sentfin_dp = proj_path / 'data/sentfin'\n",
    "newsmtsc_dp = proj_path /'data/newsmtsc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_2_dframe(path: Path | str):\n",
    "    \"\"\"\n",
    "    This functions parses data files in the ACL-14-Short format. \n",
    "        Input: \n",
    "            [pathlib.Path or str] data path\n",
    "        Output:\n",
    "            DataFrame with columns 'context' (str), 'target'(str), 'polarity'(int).\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            flines = f.read().splitlines()\n",
    "        fdict = dict(\n",
    "            context=flines[::3],\n",
    "            target=flines[1::3],\n",
    "            polarity=flines[2::3])\n",
    "        df = pd.DataFrame(fdict)\n",
    "        df['polarity'] = pd.to_numeric(df.polarity)\n",
    "        return df\n",
    "    except FileNotFoundError as err:\n",
    "        # logging statement?\n",
    "        raise err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_data_2_dframe(acl_data_path/'raw/train.raw')\n",
    "df_test = load_data_2_dframe(acl_data_path/'raw/test.raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning:\n",
    "\n",
    "Steps involved are:\n",
    "1. Removing unnecessary characters (including spurious blank spaces, references, etc.)\n",
    "2. Clean contractions\n",
    "3. Replace emojis with their textual equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emoticons(text):\n",
    "    emot_obj = emot.core.emot() \n",
    "    res = emot_obj.emoji(text)\n",
    "    locations = reversed(res['location']) \n",
    "    replace_with = reversed(res['mean'])\n",
    "    for loc, rep in zip(locations, replace_with):\n",
    "        text = text[:loc[0]] + rep + text[loc[1]:]\n",
    "\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(text):\n",
    "    # Remove links\n",
    "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Convert HTML references\n",
    "    text = re.sub('&amp', 'and', text)\n",
    "    text = re.sub('&lt', '<', text)\n",
    "    text = re.sub('&gt', '>', text)\n",
    "    # Remove new line characters\n",
    "    text = re.sub('[\\r\\n]+', ' ', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove multiple space characters\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    try:\n",
    "        return contractions.fix(text)\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "This section features EDA steps for the ACL-14 dataset. Specific steps are as follows\n",
    "1. Most common bigrams and trigrams to detect possible spurious/problematic patterns\n",
    "2. General sentiment (inferred using Vader and TextBlob) and comparisons to target polarities.\n",
    "3. Class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_ngrams(corpus, k=None, ngram=2):\n",
    "    vec = CountVectorizer(ngram_range=(ngram, ngram), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    d_wf = pd.DataFrame(words_freq, columns = ['TweetText' , 'count'])\n",
    "    d_wf.groupby('TweetText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "        kind='bar',\n",
    "        yTitle='Count',\n",
    "        linecolor='black',\n",
    "        title='Top 20 bigrams in Tweet before removing spams')\n",
    "    return words_freq[:k], d_wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_bigram = get_top_k_ngrams(df.processed_context, k=20)\n",
    "#d_trigram = get_top_k_ngrams(df.processed_context, k=20, ngram=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir_sent_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c331ed578845b93030dd13d4fb747e2ca8135fc936f4be14d6f101c1190eba5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
